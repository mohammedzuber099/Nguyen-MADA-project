---
title: "Analysis Script"
author: "Vincent Nguyen"
date: "2025-02-21"
output: html_document
---

# Setup

Load needed packages
```{r}
library(ggplot2) 
library(broom) 
library(here) 
library(glmnet)
library(MASS)
library(tidymodels)
library(dplyr)
library(rsample)
library(tibble)
library(parsnip)
library(future)
library(vip)
library(lubridate)
library(patchwork)
```

Load in processed data and set eed
```{r}
# path to data
# note the use of the here() package and not absolute paths
data_location <- here::here("data","processed-data","processeddata.rds")

# load data 
data <- readRDS(data_location)

# set seed
rngseed = 1234
set.seed(rngseed)
```

Creation of lagged predictors
```{r}
# Creation of lagged cases and lagged mobility data
data <- data %>%
  group_by(county) %>%
  arrange(date) %>%
  mutate(
    lag_1 = lag(new_cases, 1),
    lag_7 = lag(new_cases, 7),
    lag_14 = lag(new_cases, 14),
    retail_lag7 = lag(retail_and_recreation_percent_change_from_baseline, 7),
    grocery_lag7 = lag(grocery_and_pharmacy_percent_change_from_baseline, 7),
    workplace_lag7 = lag(workplaces_percent_change_from_baseline, 7),
    residential_lag7 = lag(residential_percent_change_from_baseline, 7)
  ) %>%
  ungroup()

# remove NAs as result of lagging
data <- data %>% drop_na()
```

Implementation of rolling window cross-validation
```{r}
# Arrange by date
data <- data %>%
  arrange(date, county)

# Define testing set as last 30 days for 44 counties
test_days <- 30
last_date <- max(data$date)
test_start_date <- last_date - days(test_days - 1)

# Define train and test split through dates
train_data <- data %>% filter(date < test_start_date)
test_data  <- data %>% filter(date >= test_start_date)

# Create rolling window folds for cross-validation 
rolling_folds <- rolling_origin(
  data = train_data,
  initial = 120 * 44,  # ~4 months of training
  assess  = 30 * 44,   # ~1 month of validation
  skip    = 15 * 44,   # roll forward 15 days
  cumulative = FALSE
)

# Check date ranges for each fold
train_ranges <- purrr::map(rolling_folds$splits, ~range(pull(training(.x), date)))

# Print the ranges for each fold
train_ranges

# View fold summary
fold_sizes <- purrr::map_dfr(rolling_folds$splits, function(split) {
  tibble(
    train_n = nrow(training(split)),
    test_n  = nrow(testing(split))
  )
})

print(fold_sizes)

# Check date ranges
range(train_data$date)
range(test_data$date)

```

Recipe setting for modeling
```{r}
# Recipe for baseline model
recipe_baseline <- recipe(new_cases ~ pop_density + lag_1 + lag_7 + lag_14, data = data)

# Recipe with all predictors
recipe <- recipe(new_cases ~ pop_density +
                  retail_and_recreation_percent_change_from_baseline +
                   grocery_and_pharmacy_percent_change_from_baseline + 
                   workplaces_percent_change_from_baseline +
                   residential_percent_change_from_baseline +
                     lag_1 + lag_7 + lag_14, data = data)

# Recipe with lagged versions of mobility
recipe_lag <- recipe(new_cases ~ pop_density + retail_lag7 + grocery_lag7 + residential_lag7 + workplace_lag7 + lag_1 + lag_7 + lag_14, data = data)
```

Modeling

The basic workflow is to fit a baseline model, fit the mobility model, and then fit the lagged model.

First model are linear regression models.
This code chunk fits a baseline linear regression model.
```{r}
# Define the Lasso model with tuning parameters
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

# Create the Lasso workflow with baseline recipe
base_tuned_lasso_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(lasso_spec)

# Define a grid for tuning the penalty
lasso_grid <- grid_regular(penalty(range = c(-4, 0)), levels = 30)

# Tune the Lasso model using rolling origin cv
base_tuned_lasso_res <- tune_grid(
  base_tuned_lasso_wf,
  resamples = rolling_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything", save_pred = TRUE)
)

# View tuning results
autoplot(base_tuned_lasso_res)
base_tuned_lasso_metrics <- collect_metrics(base_tuned_lasso_res)
print(base_tuned_lasso_metrics)

# Select best penalty based on R-squared
base_tuned_lasso_best <- select_best(base_tuned_lasso_res, metric = "rsq")
print(base_tuned_lasso_best)

# Finalize the Lasso model with the selected penalty
base_final_lasso_spec <- linear_reg(
  penalty = base_tuned_lasso_best$penalty,
  mixture = 1
) %>%
  set_engine("glmnet")

# Final workflow with tuned Lasso model
base_final_lasso_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(base_final_lasso_spec)

# Fit the final Lasso model on training data
base_final_lasso_fit <- base_final_lasso_wf %>%
  fit(data = train_data)

# Evaluate the final Lasso model with rolling CV
base_final_lasso_res <- fit_resamples(
  base_final_lasso_wf,
  resamples = rolling_folds,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = TRUE)
)

# View resampling performance for the final Lasso model
collect_metrics(base_final_lasso_res)

# Predict on test data using the final Lasso model
base_final_lasso_test_preds <- predict(base_final_lasso_fit, new_data = test_data)

# Combine predictions with actual values
base_final_lasso_test_results <- test_data %>%
  select(new_cases) %>%
  mutate(predicted = base_final_lasso_test_preds$.pred)

# Compute test metrics for the final Lasso model
base_final_lasso_test_metrics <- base_final_lasso_test_results %>%
  metrics(truth = new_cases, estimate = predicted)

# View the test set performance for the final Lasso model
print(base_final_lasso_test_metrics)

# View variable importance for the Lasso model
vip(base_final_lasso_fit$fit$fit)

```

This code chunk fits a linear regression model with mobility predictors.
```{r}
# Create the Lasso workflow
lasso_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_spec)

# Tune the Lasso model using time-blocked CV
lasso_res <- tune_grid(
  lasso_wf,
  resamples = rolling_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

# View tuning results
autoplot(lasso_res)
lasso_metrics <- collect_metrics(lasso_res)
print(lasso_metrics)

# Select best penalty by R-squared
lasso_best <- select_best(lasso_res, metric = "rsq")
print(lasso_best)

# Finalize the Lasso model with selected lambda
final_lasso_spec <- linear_reg(
  penalty = lasso_best$penalty,
  mixture = 1
) %>%
  set_engine("glmnet")

# Final workflow with tuned Lasso model
final_lasso_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_lasso_spec)

# Fit final model on training data
final_lasso_fit <- final_lasso_wf %>%
  fit(data = train_data)

# Evaluate final model with rolling CV
final_lasso_res <- fit_resamples(
  final_lasso_wf,
  resamples = rolling_folds,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = TRUE)
)

# View resampling performance
collect_metrics(final_lasso_res)

# Predict on test data
final_lasso_test_preds <- predict(final_lasso_fit, new_data = test_data)

# Combine predictions with actuals
final_lasso_test_results <- test_data %>%
  select(new_cases) %>%
  mutate(predicted = final_lasso_test_preds$.pred)

# Compute test metrics
final_lasso_test_metrics <- final_lasso_test_results %>%
  metrics(truth = new_cases, estimate = predicted)

# View test set performance
print(final_lasso_test_metrics)

# View variable importance
vip(final_lasso_fit$fit$fit)
```

```{r}
# Create the Lasso workflow
lag_lasso_wf <- workflow() %>%
  add_recipe(recipe_lag) %>%
  add_model(lasso_spec)

# Tune the Lasso model using time-blocked CV
lag_lasso_res <- tune_grid(
  lag_lasso_wf,
  resamples = rolling_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

# View tuning results
autoplot(lag_lasso_res)
lag_lasso_metrics <- collect_metrics(lag_lasso_res)
print(lag_lasso_metrics)

# Select best penalty by R-squared
lag_lasso_best <- select_best(lag_lasso_res, metric = "rsq")
print(lag_lasso_best)

# Finalize the Lasso model with selected lambda
lag_final_lasso_spec <- linear_reg(
  penalty = lag_lasso_best$penalty,
  mixture = 1
) %>%
  set_engine("glmnet")

# Final workflow with tuned Lasso model
lag_final_lasso_wf <- workflow() %>%
  add_recipe(recipe_lag) %>%
  add_model(lag_final_lasso_spec)

# Fit final model on training data
lag_final_lasso_fit <- lag_final_lasso_wf %>%
  fit(data = train_data)

# Evaluate final model with rolling CV
lag_final_lasso_res <- fit_resamples(
  lag_final_lasso_wf,
  resamples = rolling_folds,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = TRUE)
)

# View resampling performance
collect_metrics(lag_final_lasso_res)

# Predict on test data
lag_final_lasso_test_preds <- predict(lag_final_lasso_fit, new_data = test_data)

# Combine predictions with actuals
lag_final_lasso_test_results <- test_data %>%
  select(new_cases) %>%
  mutate(predicted = lag_final_lasso_test_preds$.pred)

# Compute test metrics
lag_final_lasso_test_metrics <- lag_final_lasso_test_results %>%
  metrics(truth = new_cases, estimate = predicted)

# View test set performance
print(lag_final_lasso_test_metrics)

# View variable importance
vip(lag_final_lasso_fit$fit$fit)


```

```{r}
# plot baseline lasso model prediction vs actual
plot_base_lasso <- base_final_lasso_test_results %>%
  ggplot(aes(x = new_cases, y = predicted)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Base Lasso",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

# plot mobility lasso model prediction vs actual
plot_lasso <- final_lasso_test_results %>%
  ggplot(aes(x = new_cases, y = predicted)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Lasso",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

# plot lagged lasso model prediction vs actual
plot_lag_lasso <- lag_final_lasso_test_results %>%
  ggplot(aes(x = new_cases, y = predicted)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Lag Lasso",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

lasso_graphs <- plot_base_lasso + plot_lasso + plot_lag_lasso +
  plot_layout(ncol = 3, widths = c(1.5, 1.5, 1.5)) &
  theme(
    plot.margin = margin(10, 10, 10, 10),
    plot.title = element_text(size = 14, hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

print(lasso_graphs)

figure_file = here("results", "figures", "lasso_graphs.png")
ggsave(
  filename = figure_file,
  plot = lasso_graphs,
  width = 16, height = 5, dpi = 300
)

```


Second, random forest models are tuned, trained, and evaluated. Best models are chosen through R-squared.

This code chunk fits and tunes a baseline RF model with no mobility predictors.
```{r}
# Define the Random Forest model with tuning parameters
forest_spec <- rand_forest(
  mode = "regression", 
  mtry = tune(),           
  min_n = tune(),          
  trees = 500              
) %>%
  set_engine("ranger", seed = rngseed)

# Create grid of hyperparameters for tuning
forest_grid <- grid_regular(
  mtry(range = c(1, 4)),  
  min_n(range = c(1, 50)), 
  levels = 10
)

# Create baseline workflow with baseline recipe
base_tuned_rf_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(forest_spec)

# Tune the Random Forest model using time-blocked CV (folds)
base_tuned_rf_res <- tune_grid(
  base_tuned_rf_wf,
  resamples = rolling_folds,                     
  grid = forest_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything")
)

# View tuning results
autoplot(base_tuned_rf_res)
base_tune_rf_metrics <- collect_metrics(base_tuned_rf_res)
print(base_tune_rf_metrics)

# Select best hyperparameters by R-squared
base_tuned_rf_best <- select_best(base_tuned_rf_res, metric = "rsq")

# Finalize model with best parameters
base_final_rf_spec <- rand_forest(
  mode = "regression",
  mtry = base_tuned_rf_best$mtry,
  min_n = base_tuned_rf_best$min_n,
  trees = 500
) %>%
  set_engine("ranger", seed = rngseed)

# Final workflow with tuned model
base_final_rf_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(base_final_rf_spec)

# Fit final model on training data
base_final_rf_fit <- base_final_rf_wf %>%
  fit(data = train_data)

# Evaluate final model with rolling CV
base_final_rf_res <- fit_resamples(
  base_final_rf_wf,
  resamples = rolling_folds,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = TRUE)
)

# View resampling performance
collect_metrics(base_final_rf_res)

# Predict on test data
base_final_rf_test_preds <- predict(base_final_rf_fit, new_data = test_data)

# Combine predictions with actuals
base_final_rf_test_results <- test_data %>%
  select(new_cases) %>%
  mutate(predicted = base_final_rf_test_preds$.pred)

# Compute test metrics
base_final_rf_test_metrics <- base_final_rf_test_results %>%
  metrics(truth = new_cases, estimate = predicted)

# View test set performance
print(base_final_rf_test_metrics)

```

This code chunk fits and tunes a RF model with mobility predictors.
```{r}
# Create workflow with baseline recipe
tuned_rf_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(forest_spec)

# Tune the Random Forest model using time-blocked CV (folds)
rf_tune_res <- tune_grid(
  tuned_rf_wf,
  resamples = rolling_folds,                     
  grid = forest_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything")
)

# View tuning results
autoplot(rf_tune_res)
rf_tune_metrics <- collect_metrics(rf_tune_res)
print(rf_tune_metrics)

# Select best hyperparameters by R-squared
rf_best <- select_best(rf_tune_res, metric = "rsq")

# Finalize model with best parameters
final_rf_spec <- rand_forest(
  mode = "regression",
  mtry = rf_best$mtry,
  min_n = rf_best$min_n,
  trees = 500
) %>%
  set_engine("ranger", seed = rngseed)

# Final workflow with tuned model
final_rf_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(final_rf_spec)

# Fit final model on training data
final_rf_fit <- final_rf_wf %>%
  fit(data = train_data)

# Evaluate final model with rolling CV
final_rf_res <- fit_resamples(
  final_rf_wf,
  resamples = rolling_folds,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = TRUE)
)

# View resampling performance
collect_metrics(final_rf_res)

# Predict on test data
final_rf_test_preds <- predict(final_rf_fit, new_data = test_data)

# Combine predictions with actuals
final_rf_test_results <- test_data %>%
  select(new_cases) %>%
  mutate(predicted = final_rf_test_preds$.pred)

# Compute test metrics
final_rf_test_metrics <- final_rf_test_results %>%
  metrics(truth = new_cases, estimate = predicted)

# View test set performance
print(final_rf_test_metrics)
```

This code chunk fits and tunes a RF model with lagged mobility predictors.
```{r}
# Create lag workflow with baseline recipe
lag_tuned_rf_wf <- workflow() %>%
  add_recipe(recipe_lag) %>%
  add_model(forest_spec)

# Tune the Random Forest model using time-blocked CV (folds)
lag_tuned_rf_res <- tune_grid(
  lag_tuned_rf_wf,
  resamples = rolling_folds,                     
  grid = forest_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything")
)

# View tuning results
autoplot(lag_tuned_rf_res)
lag_tune_rf_metrics <- collect_metrics(lag_tuned_rf_res)
print(lag_tune_rf_metrics)

# Select best hyperparameters by R-squared
lag_tuned_rf_best <- select_best(lag_tuned_rf_res, metric = "rsq")

# Finalize model with best parameters
lag_final_rf_spec <- rand_forest(
  mode = "regression",
  mtry = lag_tuned_rf_best$mtry,
  min_n = lag_tuned_rf_best$min_n,
  trees = 500
) %>%
  set_engine("ranger", seed = rngseed)

# Final workflow with tuned model
lag_final_rf_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(lag_final_rf_spec)

# Fit final model on training data
lag_final_rf_fit <- lag_final_rf_wf %>%
  fit(data = train_data)

# Evaluate final model with rolling CV
lag_final_rf_res <- fit_resamples(
  lag_final_rf_wf,
  resamples = rolling_folds,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = TRUE)
)

# View resampling performance
collect_metrics(lag_final_rf_res)

# Predict on test data
lag_final_rf_test_preds <- predict(lag_final_rf_fit, new_data = test_data)

# Combine predictions with actuals
lag_final_rf_test_results <- test_data %>%
  select(new_cases) %>%
  mutate(predicted = lag_final_rf_test_preds$.pred)

# Compute test metrics
lag_final_rf_test_metrics <- lag_final_rf_test_results %>%
  metrics(truth = new_cases, estimate = predicted)

# View test set performance
print(lag_final_rf_test_metrics)

```

```{r}
# plot baseline rf model prediction vs actual
plot_base_rf <- base_final_rf_test_results %>%
  ggplot(aes(x = new_cases, y = predicted)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Base RF",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

# plot mobility rf model prediction vs actual
plot_rf <- final_rf_test_results %>%
  ggplot(aes(x = new_cases, y = predicted)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "RF with Mobility",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

# plot lagged rf model prediction vs actual
plot_lag_rf <- lag_final_rf_test_results %>%
  ggplot(aes(x = new_cases, y = predicted)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "RF with Lagged Mobility",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

rf_graphs <- plot_base_rf + plot_rf + plot_lag_rf +
  plot_layout(ncol = 3, widths = c(1.5, 1.5, 1.5)) &
  theme(
    plot.margin = margin(10, 10, 10, 10),
    plot.title = element_text(size = 14, hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

print(rf_graphs)

figure_file = here("results", "figures", "rf_graphs.png")
ggsave(
  filename = figure_file,
  plot = rf_graphs,
  width = 16, height = 5, dpi = 300
)
```

Third, XGBoost models are tuned, trained, and evaluated. Best models are chosen through R-squared.

This code chunk fits and tunes an baseline XGBoost model with no mobility predictors.
```{r}
# Define the XGBoost model specification (baseline version)
boost_spec <- boost_tree(
  trees = 500,
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Build workflow with baseline recipe and model
base_boost_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(boost_spec)

# Tune XGBoost model using time-blocked CV (folds)
base_boost_tune_res <- tune_grid(
  base_boost_wf,
  resamples = rolling_folds,
  grid = 30,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything")
)

# View tuning results and print best model metrics
base_boost_tune_res %>%
  collect_metrics() %>%
  print()

base_best_params <- select_best(base_boost_tune_res, metric = "rsq")
print(base_best_params)

# Final XGBoost model with best hyperparameters
base_final_boost_spec <- boost_tree(
  trees = 500,
  tree_depth = base_best_params$tree_depth,
  learn_rate = base_best_params$learn_rate,
  loss_reduction = base_best_params$loss_reduction
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Final model workflow
base_final_boost_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(base_final_boost_spec)

# Fit the final model
base_final_boost_fit <- base_final_boost_wf %>%
  fit(data = train_data)

# Predict on test data and compute performance metrics
base_final_boost_preds <- predict(base_final_boost_fit, new_data = test_data)

base_final_boost_results <- tibble(
  truth = test_data$new_cases,
  predicted = base_final_boost_preds$.pred
)

base_final_boost_metrics <- base_final_boost_results %>%
  metrics(truth = truth, estimate = predicted)

# Print final performance metrics
print(base_final_boost_metrics)
```

This code chunk fits and tunes an XGBoost model with mobility predictors.
```{r}
# Build workflow with recipe and model
boost_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(boost_spec)

# Tune XGBoost model using time-blocked CV (folds)
boost_tune_res <- tune_grid(
  boost_wf,
  resamples = rolling_folds,
  grid = 30,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything")
)

# View tuning results and print best model metrics
boost_tune_res %>%
  collect_metrics() %>%
  print()

best_params <- select_best(boost_tune_res, metric = "rsq")
print(best_params)

# Final XGBoost model with best hyperparameters
final_boost_spec <- boost_tree(
  trees = 500,
  tree_depth = best_params$tree_depth,
  learn_rate = best_params$learn_rate,
  loss_reduction = best_params$loss_reduction
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Final model workflow
final_boost_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(final_boost_spec)

# Fit the final model
final_boost_fit <- final_boost_wf %>%
  fit(data = train_data)

# Predict on test data and compute performance metrics
final_boost_preds <- predict(final_boost_fit, new_data = test_data)

final_boost_results <- tibble(
  truth = test_data$new_cases,
  predicted = final_boost_preds$.pred
)

final_boost_metrics <- final_boost_results %>%
  metrics(truth = truth, estimate = predicted)

# Print final performance metrics
print(final_boost_metrics)

```

This code chunk fits and tunes an XGBoost model with lagged mobility predictors.
```{r}
# Build workflow with baseline recipe and model
lag_boost_wf <- workflow() %>%
  add_recipe(recipe_lag) %>%
  add_model(boost_spec)

# Tune XGBoost model using time-blocked CV (folds)
lag_boost_tune_res <- tune_grid(
  lag_boost_wf,
  resamples = rolling_folds,
  grid = 30,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(parallel_over = "everything")
)

# View tuning results and print best model metrics
lag_boost_tune_res %>%
  collect_metrics() %>%
  print()

lag_best_params <- select_best(lag_boost_tune_res, metric = "rsq")
print(lag_best_params)

# Final XGBoost model with best hyperparameters
lag_final_boost_spec <- boost_tree(
  trees = 500,
  tree_depth = lag_best_params$tree_depth,
  learn_rate = lag_best_params$learn_rate,
  loss_reduction = lag_best_params$loss_reduction
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Final model workflow
lag_final_boost_wf <- workflow() %>%
  add_recipe(recipe_baseline) %>%
  add_model(lag_final_boost_spec)

# Fit the final model
lag_final_boost_fit <- lag_final_boost_wf %>%
  fit(data = train_data)

# Predict on test data and compute performance metrics
lag_final_boost_preds <- predict(lag_final_boost_fit, new_data = test_data)

lag_final_boost_results <- tibble(
  truth = test_data$new_cases,
  predicted = lag_final_boost_preds$.pred
)

lag_final_boost_metrics <- lag_final_boost_results %>%
  metrics(truth = truth, estimate = predicted)

# Print final performance metrics
print(lag_final_boost_metrics)
```

```{r}
# plot baseline boost model prediction vs. actual
plot_base_boost <- base_final_boost_results %>%
  ggplot(aes(x = truth, y = predicted)) +
  geom_point(alpha = 0.6, color = "darkgreen") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Base Boost",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

# plot mobility boost model prediction vs actual
plot_boost <- final_boost_results %>%
  ggplot(aes(x = truth, y = predicted)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Boost with Mobility",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

# plot lagged boost model prediction vs actual
plot_lag_boost <- lag_final_boost_results %>%
  ggplot(aes(x = truth, y = predicted)) +
  geom_point(alpha = 0.6, color = "purple") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Boost with Lagged Mobility",
    x = "Actual Cases", y = "Predicted Cases"
  ) +
  theme_bw()

boost_graphs <- plot_base_boost + plot_boost + plot_lag_boost +
  plot_layout(ncol = 3, widths = c(1.5, 1.5, 1.5)) &
  theme(
    plot.margin = margin(10, 10, 10, 10),
    plot.title = element_text(size = 14, hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

print(boost_graphs)

figure_file = here("results", "figures", "boost_graphs.png")
ggsave(
  filename = figure_file,
  plot = boost_graphs,
  width = 16, height = 5, dpi = 300
)

```


This code chunk merges the results of the best models into a data frame.
```{r}
base_lasso_metrics <- base_final_lasso_test_metrics %>% 
  mutate(model = "LASSO (Baseline)") %>%
  rename(mean = .estimate)

lasso_metrics <- final_lasso_test_metrics %>% 
  mutate(model = "LASSO") %>%
  rename(mean = .estimate)

lag_lasso_metrics <- lag_final_lasso_test_metrics %>% 
  mutate(model = "LASSO (Lagged Mobility)") %>%
  rename(mean = .estimate)

final_boost_metrics <- final_boost_metrics %>%
  mutate(model = "XGBoost") %>%
  rename(mean = .estimate)

final_forest_metrics <- final_rf_test_metrics %>%
  mutate(model = "Random Forest") %>%
  rename(mean = .estimate)

base_final_boost_metrics <- base_final_boost_metrics %>%
  mutate(model = "XGBoost (Baseline)") %>%
  rename(mean = .estimate)

base_final_forest_metrics <- base_final_rf_test_metrics %>%
  mutate(model = "Random Forest (Baseline)") %>%
  rename(mean = .estimate)

lag_boost_metrics <- lag_final_boost_metrics %>%
  mutate(model = "XGBoost (Lagged Mobility)") %>%
  rename(mean = .estimate)

lag_forest_metrics <- lag_final_rf_test_metrics %>%
  mutate(model = "Random Forest (Lagged Mobility)") %>%
  rename(mean = .estimate)

model_metrics <- bind_rows(base_lasso_metrics, lasso_metrics, lag_lasso_metrics, base_final_boost_metrics, final_boost_metrics, lag_boost_metrics, base_final_forest_metrics, final_forest_metrics, lag_forest_metrics)

model_metrics <- model_metrics %>%
  select(-.estimator) %>%
  pivot_wider(names_from = '.metric', values_from = mean)
```

This code chunk creates a short summary table of the results from the model_metrics data frame.
```{r}
library(gt)

model_table <- model_metrics %>%
  gt() %>%
  tab_header(
    title = "Model Performance Metrics",
    subtitle = "Best values are highlighted per metric"
  ) %>%
  fmt_number(columns = c(rmse, mae), decimals = 1) %>%
  fmt_number(columns = rsq, decimals = 3) %>%
  cols_label(
    model = "Model",
    rmse = "RMSE",
    rsq = "R²",
    mae = "MAE"
  ) %>%

  # Highlight lowest RMSE
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = rmse,
      rows = rmse == min(rmse, na.rm = TRUE)
    )
  ) %>%

  # Highlight lowest MAE
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = mae,
      rows = mae == min(mae, na.rm = TRUE)
    )
  ) %>%

  # Highlight highest R²
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = rsq,
      rows = rsq == max(rsq, na.rm = TRUE)
    )
  ) %>%

  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.width = pct(100),
    table.align = "left",
    column_labels.padding = px(10),
    data_row.padding = px(6)
  )

library(gtExtras)
library(webshot2)

print(model_table)

gtsave(model_table, here("results", "figures", "models.png"))
```